{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the proximity matrix with the similarity of the samples' leaf indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(parallel=True)\n",
    "def Comparison(arr1, arr2):\n",
    "    comparison = arr1 == arr2\n",
    "    return np.sum(comparison)\n",
    "    \n",
    "    \n",
    "def Proximity_Matrix(pm, leaf_idx):\n",
    "    '''pm: initial proximity matrix with the values of 0\n",
    "       leaf_idx: the leaf index of the samples in the Random Forest'''\n",
    "    n_samples=leaf_idx.shape[0]\n",
    "    \n",
    "    #Update the matrix\n",
    "    start=time.time()\n",
    "    for i in range(n_samples-1):\n",
    "        for j in range(i+1, n_samples):\n",
    "\n",
    "            #Compare the similarity between the samples in the leaf index\n",
    "            adds = Comparison(leaf_idx[i], leaf_idx[j])\n",
    "\n",
    "            pm[i][j] += adds\n",
    "            pm[j][i] += adds\n",
    "            \n",
    "        #Demonstrate the progress    \n",
    "        if i%200==0:\n",
    "            end=time.time()\n",
    "            \n",
    "            n_total = n_samples*(n_samples-1)/2\n",
    "            n_elapsed = (n_samples-1+n_samples-1-(i+1))*(i+1)/2\n",
    "            \n",
    "            time_elapsed = (end-start)//60\n",
    "            print(\"PM progress... {}-th 200-sample is done! Elapsed time {} mins\\n\\\n",
    "            Rest time: {} mins\".format((i+1)//200, time_elapsed, (time_elapsed/n_elapsed)*n_total//60+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the initial guess of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the value frequency in DF\n",
    "@jit\n",
    "def Get_Value_Frequency(value, arr):\n",
    "    #Find the number of the desired values in the column\n",
    "    idx = np.argwhere(arr==value)\n",
    "    \n",
    "    #The sum of the desired value counts\n",
    "    value_sum = arr[idx].size\n",
    "    \n",
    "    return value_sum/arr.size\n",
    "\n",
    "\n",
    "#Caluculate weight frequency\n",
    "@jit(parallel=True)\n",
    "def Weight_Frequency(pm, row_id, P_value, F_value):\n",
    "    #Proximity of all the values\n",
    "    P_values = pm[row_id].sum()\n",
    "    W_value = P_value/P_values\n",
    "\n",
    "    #Weight Frequency of the value\n",
    "    return F_value*W_value\n",
    "\n",
    "\n",
    "#4. Calculate the final guess\n",
    "def Refine_Guess(row_id, column_id, df, pm, nominal):\n",
    "    #column_id: the column needed to evaluate\n",
    "    #row_id: the sample which have missing values in the column\n",
    "    #df: the original dataset with missing values\n",
    "    #pm:the proximity matrix\n",
    "    \n",
    "    #All the unique values in the original column\n",
    "    arr_unique = np.unique(df[:,column_id])\n",
    "    values = arr_unique[~np.isnan(arr_unique)]\n",
    "    \n",
    "    #For nominal feature, use weight frequency\n",
    "    #For numeric feature, use weight average of the values\n",
    "    WF_values=[]\n",
    "    VA_values=0\n",
    "    for value in tqdm(values):\n",
    "        #Frequency of the value in the column\n",
    "        F_value = Get_Value_Frequency(value=value, arr=df[:,column_id])\n",
    "    \n",
    "        #Weight in proximity matrix\n",
    "            #Proximity of the value\n",
    "        P_value=0\n",
    "            \n",
    "        for v in range(len(df[:,column_id])):\n",
    "            if df[:,column_id][v] == value:\n",
    "                #Get the proximity from PM by row_id and column id\n",
    "                P_value+=pm[row_id][v]\n",
    "            \n",
    "            #Proximity of all the values\n",
    "        WF_values.append(Weight_Frequency(pm, row_id, P_value, F_value))\n",
    "        \n",
    "            #Weight Average of the value\n",
    "        VA_values+=value*W_value\n",
    "\n",
    "    #Find the highest WF_value's index for nominal feature\n",
    "    idx = np.argmax(WF_values)\n",
    "    \n",
    "    #Return the final guessed value\n",
    "    if nominal==True:\n",
    "        return values[idx]\n",
    "    else:\n",
    "        return VA_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Repeat 1~ 4 steps\n",
    "#Unitl convergence between current guesses and the last guesses\n",
    "#or difference below some tolerance\n",
    "def Reat_Until_Converge(df, pm, max_iters, toler):\n",
    "    for itera in range(max_iters):\n",
    "        #Define a tempo array for saving the values of the dataframe with initial guesses\n",
    "        df_rf = df.copy().to_numpy()\n",
    "\n",
    "        values_last_guessed = []\n",
    "        values_current_guess = []\n",
    "\n",
    "        for row_id in tqdm(list(idex_nans.keys())):\n",
    "            for column_id in idex_nans[int(row_id)]:\n",
    "                #Get the last guess\n",
    "                value_last_guessed = df_rf[int(row_id), column_id]\n",
    "                values_last_guessed.append(value_last_guessed)\n",
    "\n",
    "                #Get the next guess\n",
    "                value_current_guess = Refine_Guess(int(row_id), column_id, df_rf, pm, nominal=True)\n",
    "                values_current_guess.append(value_current_guess)\n",
    "\n",
    "                #Change the dataset according to current guess\n",
    "                df_rf[int(row_id), column_id] = value_current_guess\n",
    "\n",
    "        values_last_guessed=np.array(values_last_guessed)\n",
    "        values_current_guess=np.array(values_current_guess)\n",
    "        \n",
    "\n",
    "        error = np.sum(np.abs(values_last_guessed-values_current_guess))/values_last_guessed.shape[0]\n",
    "        print(\"Iteration: \", itera, \"  error: \", error)\n",
    "               \n",
    "        if error <= toler:\n",
    "            return df_rf, pm, error\n",
    "        \n",
    "    #Return pm, df\n",
    "    return df_rf, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
